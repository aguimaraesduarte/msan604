data <- read.table("sp500.txt", header = T)
sp500 <- data$AdjClose
sp500 <- ts(sp500)
par(mfrow = c(1,1))
plot(sp500, main = "S&P 500 Adjusted Closing Prices", ylab = "Closing Cost")
# Transform sp500 to look at percent changes rather than closing prices
ch <- diff(log(sp500))
plot(ch, main = "Day-Over-Day Log-Change in S&P 500 Closing Prices", ylab = "Log-Change") #this is the time series we want to model
# Check whether this is stationary
library(forecast)
ndiffs(ch)
acf(ch, lag.max = 120, main = "ACF Plot of Log-Change Series") # -> ARMA is probably fine
# this time series passes the ADF test, and so we believe it is stationary,
# and so we can fit an ARMA model. Examine ACF/PACF plots for orders
par(mfrow=c(2,1))
acf(ch, lag.max = 120, main = "ACF Plot of Log-Change Series")
pacf(ch, lag.max = 120, main = "PACF Plot of Log-Change Series")
# there does not appear to be any seasonality, and perhaps p = q = 1, or just q = 1?
m1 <- arima(ch, order = c(1,0,1), include.mean = F)
summary(m1)
# residual diagnostics:
tsdiag(m1) # -> heteroskedastic, non-correlated-ish
par(mfrow=c(2,1))
acf(m1$residuals, main = "ACF of ARMA(1,1) Residuals")
acf(abs(m1$residuals), main = "ACF of Absolute ARMA(1,1) Residuals") # -> GARCH residuals!
par(mfrow=c(1,1))
qqnorm(m1$residuals, main = "Normal QQ-Plot of ARMA(1,1) Residuals")
qqline(m1$residuals, col = "red") # -> terrible, maybe a t-dist would be better
hist(m1$residuals/sqrt(m1$sigma2),freq = F, main = "Histogram of Standardized Resdiuals with N(0,1) Density Curve Overlayed", xlab = "")
curve(dnorm, -5, 10, add = T, col = "red")
# It appears as though the Normality assumption, and the heteroscedasticity assumption are not met. Clearly
# there is volatility clustering, so let's consider modeling the ARMA(1,1) residuals with a
# GARCH process. To do this, we will work with the ARMA(1,1) residuals as the time series of interest
# and try to fit a GARCH model to this.
r <- m1$residuals
plot(r, main = "ARMA(1,1) Residuals", ylab = "Residuals")
# To choose the orders k and l, we need to look at the ACF and PACF plots of |r| or r^2
par(mfrow=c(2,1))
acf(abs(r), lag.max = 120, main = "ACF plot of |r|, where r = ARMA(1,1) residuals")
pacf(abs(r), lag.max = 120, main = "PACF plot of |r|, where r = ARMA(1,1) residuals")
# There is clearly decay on both of these plots suggesting that k,l >= 1. As a simple place to begin
# modeling, try k = l = 1. To actually fit an ARMA + GARCH model, we can use either:
# -- the garchFit function in the 'fGARCH' library, or
# -- the ugarchfit function in the 'rugarch' library
library(fGarch)
# do garchFit on only garch(1,1) first, to verify that adding arma(1,1) effectively improves the fit
m2.1 <- garchFit(formula = ~arma(1,1) + garch(1,1), data = ch, include.mean = F)
summary(m2.1)
library(rugarch)
? ugarchspec
? ugarchfit
m2.spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1,1), submodel = "GARCH"), #in R, (l,k) instead of (k,l)!!!
mean.model = list(armaOrder = c(1,1), include.mean = F, external.regressors = NULL),
distribution.model = "norm")
m2.2 <- ugarchfit(spec = m2.spec, data = ch)
m2.2
# Let's do some residual diagnostics for the ARMA(1,1)+GARCH(1,1) model:
par(mfrow=c(1,1))
plot(m2.1)
hist(m2.1@residuals/m2.1@sigma.t, freq = F, main = "Histogram of Standardized ARMA(1,1)+GARCH(1,1) Residuals with N(0,1) Density Curve Overlayed", xlab = "")
curve(dnorm, -6, 4, add = T, col = "red")
# This suggests a slightly skewed conditional density with heavier tails (like the skewed t-distribution
# or skewed GED) may be appropriate. Try reffitting with this assumption:
m3.1 <- garchFit(formula = ~arma(1,1) + garch(1,1), data = ch, include.mean = F, cond.dist = "sstd")
summary(m3.1)
plot(m3.1)
m3.spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1,1), submodel = "GARCH"),
mean.model = list(armaOrder = c(1,1), include.mean = F, external.regressors = NULL),
distribution.model = "sstd")
m3.2 <- ugarchfit(spec = m3.spec, data = ch)
m3.2
plot(m3.2)
# The ARMA(1,1)+GARCH(1,1) model fit assuming a skewed t-distribution appears to be best.
# Let's forecast with it.
predict(m3.1, n.ahead = 1)
ugarchforecast(m3.2, n.ahead = 1)
# The code above calculates a one-step-ahead forecast for both the mean and the standard deviation
# If we want to predict further into the future, we should employ a moving or rolling window approach
# Also note that these forecasts are of day-to-day changes in sp500 index value. If we want to forecast
# the raw sp500 stock prices, we need to untransform
? diffinv
# exp(diffinv(ch, xi=sp500[1])) # to "undo" log + differencing
# So why use one package over the other?
# With ugarchspec, you can add external regressors into the mean model like, for example, dummy
# variables. This is how we can account for seasonality in these models. It is a round about way of
# fitting a SARIMA + GARCH model (which does not currently exist in a user-friendly way in R).
rm(list=ls())
cat("\014")
par(mfrow=c(1,1))
?predict
?adf.test
rm(list=ls())
cat("\014")
data <- read.table("sp500.txt", header = T)
sp500 <- data$AdjClose
sp500 <- ts(sp500)
par(mfrow = c(1,1))
plot(sp500, main = "S&P 500 Adjusted Closing Prices", ylab = "Closing Cost")
# Transform sp500 to look at percent changes rather than closing prices
ch <- diff(log(sp500))
plot(ch, main = "Day-Over-Day Log-Change in S&P 500 Closing Prices", ylab = "Log-Change") #this is the time series we want to model
# Check whether this is stationary
library(forecast)
ndiffs(ch)
acf(ch, lag.max = 120, main = "ACF Plot of Log-Change Series") # -> ARMA is probably fine
# this time series passes the ADF test, and so we believe it is stationary,
# and so we can fit an ARMA model. Examine ACF/PACF plots for orders
par(mfrow=c(2,1))
acf(ch, lag.max = 120, main = "ACF Plot of Log-Change Series")
pacf(ch, lag.max = 120, main = "PACF Plot of Log-Change Series")
# there does not appear to be any seasonality, and perhaps p = q = 1, or just q = 1?
m1 <- arima(ch, order = c(1,0,1), include.mean = F)
summary(m1)
# residual diagnostics:
tsdiag(m1) # -> heteroskedastic, non-correlated-ish
par(mfrow=c(2,1))
acf(m1$residuals, main = "ACF of ARMA(1,1) Residuals")
acf(abs(m1$residuals), main = "ACF of Absolute ARMA(1,1) Residuals") # -> GARCH residuals!
par(mfrow=c(1,1))
qqnorm(m1$residuals, main = "Normal QQ-Plot of ARMA(1,1) Residuals")
qqline(m1$residuals, col = "red") # -> terrible, maybe a t-dist would be better
hist(m1$residuals/sqrt(m1$sigma2),freq = F, main = "Histogram of Standardized Resdiuals with N(0,1) Density Curve Overlayed", xlab = "")
curve(dnorm, -5, 10, add = T, col = "red")
# It appears as though the Normality assumption, and the heteroscedasticity assumption are not met. Clearly
# there is volatility clustering, so let's consider modeling the ARMA(1,1) residuals with a
# GARCH process. To do this, we will work with the ARMA(1,1) residuals as the time series of interest
# and try to fit a GARCH model to this.
r <- m1$residuals
plot(r, main = "ARMA(1,1) Residuals", ylab = "Residuals")
# To choose the orders k and l, we need to look at the ACF and PACF plots of |r| or r^2
par(mfrow=c(2,1))
acf(abs(r), lag.max = 120, main = "ACF plot of |r|, where r = ARMA(1,1) residuals")
pacf(abs(r), lag.max = 120, main = "PACF plot of |r|, where r = ARMA(1,1) residuals")
# There is clearly decay on both of these plots suggesting that k,l >= 1. As a simple place to begin
# modeling, try k = l = 1. To actually fit an ARMA + GARCH model, we can use either:
# -- the garchFit function in the 'fGARCH' library, or
# -- the ugarchfit function in the 'rugarch' library
library(fGarch)
# do garchFit on only garch(1,1) first, to verify that adding arma(1,1) effectively improves the fit
m2.1 <- garchFit(formula = ~arma(1,1) + garch(1,1), data = ch, include.mean = F)
summary(m2.1)
rm(list=ls())
cat("\014")
source('~/Documents/Module2/msan604/MSAN604 Final Exam Part 2 - template.R')
library(astsa)
library(timeSeries)
library(fGarch)
install.packages("astsa")
library(astsa)
data(jj)
plot(jj, main = "Johnson & Johnson Quarterly Revenue from 1960-1980", ylab = "Quarterly Revenue")
par(mfrow=c(1,1))
plot(jj, main = "Johnson & Johnson Quarterly Revenue from 1960-1980", ylab = "Quarterly Revenue")
ljj <- log(jj)
plot(ljj, main = "Log-transformed Johnson & Johnson Quarterly Revenue from 1960-1980",
ylab = "Quarterly Revenue")
jj
plot(decompose(log(jj)))
t <- time(log(jj)) # Extracting time as the explanatory variate from the time series framework of data
t
cycle(log(jj)) # Introducing month as the season
month <- as.factor(cycle(log(jj))) # Introducing month as the season
reg0 <- lm(log(jj)~t)
quarter <- as.factor(cycle(log(jj))) # Introducing quarter as the season
reg0 <- lm(log(jj)~t+quarter) # Fitting the model reg2 with linear trend and seasonal effect
summary(reg0
plot(log(jj))
summary(reg0)
plot(log(jj))
points(t,predict.lm(reg0),type='l',col='red')
acf(reg0$residuals)
acf(reg0$residuals, main = "ACF of residuals")
paste("text")
acf(reg0$residuals, lag.max = 48, main = "ACF of residuals")
acf(log(jj))
m0 <- sarima(log(jj), order = c(1,1,1), seasonal = list(order = c(1,1,0), period = 4))
m1 <- sarima(log(jj), order = c(0,1,1), seasonal = list(order = c(1,1,0), period = 4))
m2 <- sarima(log(jj), order = c(1,1,0), seasonal = list(order = c(1,1,0), period = 4))
m0 <- arima(log(jj), order = c(1,1,1), seasonal = list(order = c(1,1,0), period = 4))
m1 <- arima(log(jj), order = c(0,1,1), seasonal = list(order = c(1,1,0), period = 4))
m2 <- arima(log(jj), order = c(1,1,0), seasonal = list(order = c(1,1,0), period = 4))
summary(m0)
m0$aic
m1$aic
m2$aic
paste("We want the model that has the lowest AIC.")
paste("The lowest AIC is obtained for m1 (SARIMA(0,1,1)x(1,1,0)[4])), for which AIC = -150.9134.")
acf(m1$residuals, lag.max = 48, main = "ACF of residuals for SARIMA model")
f <- forecast(m1, h=8, level=0.95)
f
l <- ts(f$lower, start = c(1981, 1), frequency = 4)  #95% PI LL
h <- ts(f$upper, start = c(1981, 1), frequency = 4) #95% PI UL
pred <- f$mean #predictions
par(mfrow=c(1,1))
plot(jj)
jj
f
plot(jj, xlim=c(1960, 1984), ylim=c(0, 25), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1982, lwd = 1, col = "black")
abline(v = 1980, lwd = 1, col = "black")
abline(v = 1981, lwd = 1, col = "black")
plot(jj, xlim=c(1960, 1984), ylim=c(0, 25), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1981, lwd = 1, col = "black")
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
points(jj, type = "l", col = "orange")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
abline(v = 1980.75, lwd = 1, col = "black")
plot(jj, xlim=c(1960, 1984), ylim=c(0, 25), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.75, lwd = 1, col = "black")
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
1.75/2
abline(v = 1980.875, lwd = 1, col = "black")
plot(jj, xlim=c(1960, 1984), ylim=c(0, 25), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black")
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
plot(jj, xlim=c(1960, 1984), ylim=c(0, 25), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
m.hw <- HoltWinters(x = jj, seasonal = "mult")
m.hw$alpha #0.2865475
m.hw$beta #0.01290944
m.hw$gamma #0.5566387
plot(m.hw)
legend("topleft", legend = c("Observed", "Fitted"), lty = 1, col = c("black", "red"), cex = 1)
paste("With this model, we get alpha = 0.07910107, beta = 0.8764919, gamma = 0.7916951.")
f <- forecast(m.hw, h=8, level=0.95)
l <- ts(f$lower, start = c(1981, 1), frequency = 4)  #95% PI LL
h <- ts(f$upper, start = c(1981, 1), frequency = 4) #95% PI UL
pred <- f$mean #predictions
par(mfrow=c(1,1))
plot(jj, xlim=c(1960, 1984), ylim=c(0, 25), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(pred, type = "l", col = "blue")
points(l, type = "l", col = "red")
points(h, type = "l", col = "red")
plot(jj, xlim=c(1960, 1984), ylim=c(0, 22), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(pred, type = "l", col = "blue")
points(l, type = "l", col = "red")
points(h, type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
points(f$fitted, type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
plot(jj, xlim=c(1960, 1984), ylim=c(0, 22), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(pred, type = "l", col = "blue")
points(l, type = "l", col = "red")
points(h, type = "l", col = "red")
points(f$fitted, type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
data(MSFT)
ts.plot(MSFT$Close, main = "Daily Microsoft Closing Prices \n from 27/09/2000 - 27/09/2001", ylab = "Daily Closing Price")
ch <- diff(log(MSFT))
plot(ch, main = "Day-Over-Day Log-Change in MSFT Closing Prices", ylab = "Log-Change") #this is the time series we want to model
ch <- diff(log(MSFT$Close))
plot(ch, main = "Day-Over-Day Log-Change in MSFT Closing Prices", ylab = "Log-Change") #this is the time series we want to model
ts.plot(ch, main = "Day-Over-Day Log-Change in MSFT Closing Prices", ylab = "Log-Change") #this is the time series we want to model
ch <- diff(log(MSFT$Close))
ts.plot(ch, main = "Day-Over-Day Log-Change in MSFT Closing Prices", ylab = "Log-Change") #this is the time series we want to model
?garch
m1 <- garchFit(formula = ~garch(1,0), data = ex1, include.mean = F)
m2 <- garchFit(formula = ~garch(1,1), data = ex1, include.mean = F)
m3 <- garchFit(formula = ~garch(2,1), data = ex1, include.mean = F)
m4 <- garchFit(formula = ~garch(1,2), data = ex1, include.mean = F)
m1 <- garchFit(formula = ~garch(1,0), data = ch, include.mean = F)
m2 <- garchFit(formula = ~garch(1,1), data = ch, include.mean = F)
m3 <- garchFit(formula = ~garch(2,1), data = ch, include.mean = F)
m4 <- garchFit(formula = ~garch(1,2), data = ch, include.mean = F)
summary(m1)
summary(m2)
summary(m3)
summary(m4)
summary(m1)
summary(garchFit(formula = ~garch(0,0), data = ch, include.mean = F))
summary(m2)
plot(m4)
m4@residuals
e <- m4@residuals # residuals
m4@sigma.t
?m4
?garchFit
r <- e/sqrt(m4@h.t) # standardized residuals
# Plot these
plot(r, main="Standardized Residuals", ylab="")
plot(r, main="Standardized Residuals")
abline(h=0, col="red")
plot(m4)
plot(r, main="Standardized Residuals", type="l")
abline(h=0, col="red")
plot(r, main="Sequence Plot of Standardized Residuals", type="l", ylab = "Standardized Residuals")
abline(h=0, col="red")
acf(r, lag.max = 48, main = "ACF of Standardized Residuals")
acf(r^2, lag.max = 48, main = "ACF of Squared Standardized Residuals")
plot(m4)
qqplot(r, main = "QQ-Plot of Standardized Residuals")
qqnorm(r, main = "QQ-Plot of Standardized Residuals")
qqline(r)
qqnorm(r, main = "QQ-Plot of Standardized Residuals")
qqline(r, col = "red")
plot(r, main="Sequence Plot of Standardized Residuals", type="l", ylab = "Standardized Residuals")
abline(h=0, col="red")
acf(r, lag.max = 48, main = "ACF of Standardized Residuals")
acf(r^2, lag.max = 48, main = "ACF of Squared Standardized Residuals")
qqnorm(r, main = "QQ-Plot of Standardized Residuals")
qqline(r, col = "red")
ljj <- log(jj)
plot(ljj, main = "Log-transformed Johnson & Johnson Quarterly Revenue from 1960-1980",
ylab = "Quarterly Revenue")
ylab = "Log-Quarterly Revenue")
plot(ljj, main = "Log-transformed Johnson & Johnson Quarterly Revenue from 1960-1980",
ylab = "Log-transformed Quarterly Revenue")
plot(decompose(log(jj)))
t <- time(log(jj)) # Extracting time as the explanatory variate from the time series framework of data
quarter <- as.factor(cycle(log(jj))) # Introducing quarter as the season
reg0 <- lm(log(jj)~t+quarter) # Fitting the model reg0 with linear trend and seasonal effect
summary(reg0)
plot(log(jj))
points(t, predict.lm(reg0), type='l', col='red')
acf(reg0$residuals, lag.max = 48, main = "ACF of residuals for decomposition model")
paste("This ACF shows that the residuals appear to be correlated.")
paste("We see significant spikes as well as a wave-like pattern.")
paste("There seems to be trend and seasonality that need to be differenced.")
m0 <- arima(log(jj), order = c(1,1,1), seasonal = list(order = c(1,1,0), period = 4))
m1 <- arima(log(jj), order = c(0,1,1), seasonal = list(order = c(1,1,0), period = 4))
m2 <- arima(log(jj), order = c(1,1,0), seasonal = list(order = c(1,1,0), period = 4))
m0$aic
m1$aic
m2$aic
paste("For this model, we have p=0, q=1, d=1 (within-season), and P=1, Q=0, D=1 (between-season), and s=4")
acf(m1$residuals, lag.max = 48, main = "ACF of residuals for SARIMA model")
paste("We see no significant spikes at any lag, so the residuals seem to be uncorrelated.")
paste("I would prefer to use this model as opposed to the decomposition model for forecasting
future quarterly revenue, as the residuals are not autocorrelated, i.e. they behave like
white noise error.")
acf(reg0$residuals, lag.max = 48, main = "ACF of residuals for decomposition model")
paste("This ACF shows that the residuals appear to be correlated.")
paste("We see significant spikes as well as a wave-like pattern.")
paste("There seems to be trend and seasonality that need to be accounted for.")
f <- forecast(m1, h=8, level=0.95)
l <- ts(f$lower, start = c(1981, 1), frequency = 4)  #95% PI LL
h <- ts(f$upper, start = c(1981, 1), frequency = 4) #95% PI UL
pred <- f$mean #predictions
par(mfrow=c(1,1))
plot(jj, xlim=c(1960, 1984), ylim=c(0, 25), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
plot(jj, xlim=c(1960, 1984), ylim=c(0, 26), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
plot(jj, xlim=c(1960, 1984), ylim=c(0, 27), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
plot(jj, xlim=c(1960, 1984), ylim=c(0, 27), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
plot(jj, xlim=c(1960, 1984), ylim=c(0, 28), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(exp(pred), type = "l", col = "blue")
points(exp(l), type = "l", col = "red")
points(exp(h), type = "l", col = "red")
points(exp(f$fitted), type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
m.hw <- HoltWinters(x = jj, seasonal = "mult")
m.hw$alpha # 0.07910107
m.hw$beta # 0.8764919
m.hw$gamma # 0.7916951
plot(m.hw)
legend("topleft", legend = c("Observed", "Fitted"), lty = 1, col = c("black", "red"), cex = 1)
paste("With this model, we get alpha = 0.07910107, beta = 0.8764919, gamma = 0.7916951.")
f <- forecast(m.hw, h=8, level=0.95)
l <- ts(f$lower, start = c(1981, 1), frequency = 4)  #95% PI LL
h <- ts(f$upper, start = c(1981, 1), frequency = 4) #95% PI UL
pred <- f$mean #predictions
par(mfrow=c(1,1))
plot(jj, xlim=c(1960, 1984), ylim=c(0, 22), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(pred, type = "l", col = "blue")
points(l, type = "l", col = "red")
points(h, type = "l", col = "red")
points(f$fitted, type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
plot(jj, xlim=c(1960, 1984), ylim=c(0, 21), main = "Johnson & Johnson Quarterly Revenue",
ylab = "Quarterly Revenue", xlab = "Time")
abline(v = 1980.875, lwd = 1, col = "black", lty = 2)
points(pred, type = "l", col = "blue")
points(l, type = "l", col = "red")
points(h, type = "l", col = "red")
points(f$fitted, type="l", col = "green")
legend("topleft", legend = c("Observed", "Fitted", "Predicted", "95% PI"),
lty = 1, col = c("black", "green", "blue", "red"), cex = 1)
data(MSFT)
ts.plot(MSFT$Close, main = "Daily Microsoft Closing Prices \n from 27/09/2000 - 27/09/2001", ylab = "Daily Closing Price")
## Using the difference-log transformation, transform this data into daily returns, and
## construct a time series plot of these daily returns.
ch <- diff(log(MSFT$Close))
ts.plot(ch, main = "Day-Over-Day Log-Change in MSFT Closing Prices", ylab = "Log-Change")
ts.plot(MSFT$Close, main = "Daily Microsoft Closing Prices \n from 27/09/2000 - 27/09/2001", ylab = "Daily Closing Price")
## Using the difference-log transformation, transform this data into daily returns, and
## construct a time series plot of these daily returns.
ch <- diff(log(MSFT$Close))
ts.plot(ch, main = "Day-Over-Day Log-Change in MSFT Closing Prices", ylab = "Log-Change")
?garch
?garchFit
m1 <- garchFit(formula = ~garch(1,0), data = ch, include.mean = F)
m2 <- garchFit(formula = ~garch(1,1), data = ch, include.mean = F)
m3 <- garchFit(formula = ~garch(2,1), data = ch, include.mean = F)
m4 <- garchFit(formula = ~garch(1,2), data = ch, include.mean = F)
paste("The model with lowest AIC is GARCH(2,1).")
paste("The model with highest log-likelihood is GARCH(2,1).")
paste("However, this model has two coefficients (omega, beta1) that are not significant.")
paste("The optimal model is still GARCH(2,1), as it has lowest AIC and highest log-likelihood.")
e <- m4@residuals # residuals
r <- e/sqrt(m4@h.t) # standardized residuals
# Standardized Residuals
plot(r, main="Sequence Plot of Standardized Residuals", type="l", ylab = "Standardized Residuals")
abline(h=0, col="red")
# ACF of Standardized Residuals
acf(r, lag.max = 48, main = "ACF of Standardized Residuals")
# ACF of Squared Standardized Residuals
acf(r^2, lag.max = 48, main = "ACF of Squared Standardized Residuals")
# QQ-Plot of Standardized Residuals
qqnorm(r, main = "QQ-Plot of Standardized Residuals")
qqline(r, col = "red")
## Using these plots comment on whether the following residual assumptions appear to be met:
## Zero Mean
paste("The sequence plot of standardized residuals shows that they seem to have zero-mean.")
## Homoscedastic
paste("The sequence plot of standardized residuals shows that they seem to be homoskedastic.")
paste("There may be a higher variability in the beginning (up to index ~50), but it seems reasonably ok in general.")
## Uncorrelated
paste("The ACF of standardized residuals shows no significant spikes, so they are not correlated.")
paste("The ACF of squared standardized residuals shows no significant spikes (except for one at lag 29),
so their norms are also not correlated.")
source('~/Documents/Module2/msan604/MSAN604 Final Exam Part 2 - aguimaraesduarte.R')
24/5
setwd("~/Documents/Module2/msan604/AP")
# Load libraries
library(tseries)
library(forecast)
###################################
# Read data
train <- read.csv("train.csv")
test <- read.csv("test.csv")
train
length(train)
train_Bankruptcy <- ts(train$Bankruptcy_Rate, c(1987, 1), frequency = 12)
train_Unemployment <- ts(train$Unemployment_Rate, c(1987, 1), frequency = 12)
train_Population <- ts(train$Population, c(1987, 1), frequency = 12)
train_HPI <- ts(train$House_Price_Index, c(1987, 1), frequency = 12)
test_Unemployment <- ts(test$Unemployment_Rate, c(2011, 1), frequency = 12)
test_Population <- ts(test$Population, c(2011, 1), frequency = 12)
test_HPI <- ts(test$House_Price_Index, c(2011, 1), frequency = 12)
length(train_HPI)
288/5
288/12
400/24
