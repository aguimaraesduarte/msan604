\documentclass[]{article}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{framed}
\usepackage{graphicx,grffile}
\usepackage{fancyvrb}
\usepackage{multirow}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{MSAN 604 - Homework 2}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Andre Guimaraes Duarte}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{November 17, 2016}
  
% Redefines (sub)paragraphs to behave more like section*s
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\null\hfill\begin{tabular}[t]{r@{}}
  \textbf{\LARGE Andre Guimaraes Duarte} \\
  \textbf{\Large 20406263}
\end{tabular}
%\maketitle

\begin{center}
\Huge
MSAN 604 - Homework 2

\Large
November 17, 2016

\normalsize
\end{center}

\section{Textbook Problems}
\paragraph{3.1}
Determine which of the following ARMA processes are causal and which of
them are invertible. (In each case $\{Z_t\}$ denotes white noise.)

a. $X_t + 0.2 X_{t-1} - 0.48 X_{t-2} = Z_t$

\color{blue}
We have 

$X_t + 0.2 X_{t-1} - 0.48 X_{t-2} = Z_t \Leftrightarrow \phi^2(B)X_t = Z_t$

with $\phi^2(z) = 1 + 0.2 z - 0.48 z^2$.

The roots of $\phi(z)$ are $z = \frac{-0.2 \pm \sqrt{(0.2)^2 + 4(0.48)^2}}{2(-0.48)} = \frac{0.2 \pm 1.4}{0.96}$. We get $z_1 = -1.25$ and $z_2 = 5/3$.

We have $|z_1| > 1$ and $|z_2| > 1$, so the process is \textbf{stationary}.

In addition, we have $\theta (z) = 1 \neq 0\ \forall z$. So the process is \textbf{invertible}.
\color{black}

b. $X_t + 1.9 X_{t-1} + 0.88 X_{t-2} = Z_t + 0.2 Z_{t-1} + 0.7 Z_{t-2}$

\color{blue}
We have

$X_t + 1.9 X_{t-1} + 0.88 X_{t-2} = Z_t + 0.2 Z_{t-1} + 0.7 Z_{t-2} \Leftrightarrow \phi^2(B)X_t = \theta^2(B)Z_t$

with $\phi^2(z) = 1 + 1.9 z + 0.88 z^2$ and $\theta^2(z) = 1 + 0.2 z + 0.7 z$.

The roots of $\phi^2(z)$ are $z = \frac{-1.9 \pm \sqrt{(1.9)^2 - 4(0.88)}}{2(0.88)} = \frac{-1.9 \pm \sqrt{0.09}}{1.76}$. We get $z_1 = -10/11$ and $z_2 = -5/4$.

We have $|z_1| < 1$, so the process is \textbf{not stationary}.

The roots of $\theta^2(z)$ are $z = \frac{-0.2 \pm \sqrt{(0.2)^2 - 4(0.7)}}{2(0.7)} = \frac{-0.2}{1.4} \pm i\frac{\sqrt{2.76}}{1.4}$.

We have $|z| = \sqrt{(0.2/1.4)^2 + (\sqrt{2.76}/1.4)^2} \simeq 1.2 > 1$. So the process is \textbf{invertible}.
\color{black}

c. $X_t + 0.6 X_{t-1} = Z_t + 1.2 Z_{t-1}$

\color{blue}
We have

$X_t + 0.6 X_{t-1} = Z_t + 1.2 Z_{t-1} \Leftrightarrow \phi(B)X_t = \theta(B)Z_t$

with $\phi(z) = 1 + 0.6 z$ and $\theta(z) = 1 + 1.2 z$.

The root of $\phi(z)$ is $z = \frac{-1}{0.6}$.

We have $|z| > 1$, so the process is \textbf{stationary}.

The root of $\theta(z)$ is $z = \frac{-1}{1.2}$.

We have $|z| < 1$, so the process is \textbf{not invertible}.
\color{black}

d. $X_t + 1.8 X_{t-1} + 0.81 X_{t-2} = Z_t$

\color{blue}
We have

$X_t + 1.8 X_{t-1} + 0.81 X_{t-2} = Z_t \Leftrightarrow \phi^2(B)X_t = Z_t$

with $\phi^2(z) = 1 + 1.8 z + 0.81 z^2$.

The root of $\phi^2(z)$ is $z = \frac{-1.8 \pm \sqrt{(1.8)^2 - 4(0.81)}}{2(0.81)} = \frac{-1.8 \pm \sqrt{3.24-3.24}}{1.62} = -10/9$.

We have $|z| > 1$, so the process is \textbf{causal}.

In addition, we have $\theta (z) = 1$. So the process is \textbf{invertible}.
\color{black}

e. $X_t + 1.6 X_{t-1} = Z_t - 0.4 Z_{t-1} + 0.04 Z_{t-2}$

\color{blue}
We have

$X_t + 1.6 X_{t-1} = Z_t - 0.4 Z_{t-1} + 0.04 Z_{t-2} \Leftrightarrow \phi(B)X_t = \theta^2(B)Z_t$

with $\phi(z) = 1 + 1.6 z$ and $\theta^2(z) = 1 - 0.4 z + 0.04 z^2$.

The root of $\phi(z)$ is $z = -1/1.6 < 1$, so the process is \textbf{not stationary}.

The root of $\theta(z)$ is $z = \frac{0.4 \pm \sqrt{(0.4)^2 - 4(0.04)}}{2(0.04)} = \frac{0.4 \pm \sqrt{0.16-0.16}}{0.08} = 5 > 1$. So the process is \textbf{invertible}.


\color{black}

\paragraph{1.10}
If $m_t = \sum^p_{k=0} c_k t^k,\ t = 0, \pm 1, \ldots$, show that $\nabla m_t$ is a polynomial of degree $p - 1$ in $t$ and hence that $\nabla^{p+1} m_t = 0$.

\color{blue}
We have $\nabla m_t = (1-B)m_t = m_t - m_{t-1} = \sum^p_{k=0} c_k t^k - \sum^p_{k=0} c_k (t-1)^k$.

It can easily be seen that the terms of order $p$ cancel each other out ($(c_p - c_p)t^p = 0$). 

In addition, the term of order $p-1$ is $p c_{p-1} \neq 0$. So the resulting polynomial is of degree $p-1$.

Therefore, every time the difference operator is applied to $m_t$, we get a polynomial of one less degree than $m_t$. As a result, we can have that $\nabla^{p} m_t$ is a polynomial of degree 0 (i.e. a constant). Therefore, we conclude that $\nabla^{p+1} m_t = 0$.

\color{black}

\paragraph{1.15}
Let $\{Y_t\}$ be a stationary process with mean zero and let $a$ and $b$ be constants.

a. If $X_t = a + bt + s_t + Y_t$, where $s_t$ is a seasonal component with period 12, show that $\nabla \nabla_{12} X_t = (1 - B)(1 - B^{12})X_t$ is stationary and express its autocovariance function in terms of that of $\{Y_t\}$.

\color{blue}
Let's call $W_t = \nabla \nabla_{12} X_t$.

We have

\begin{tabular}{ccl}
$W_t$ & = & $\nabla \nabla_{12} X_t$\\
      & = & $(1-B)(1-B^{12})X_t$\\
      & = & $(1-B)(X_t - X_{t-12})$\\
      & = & $X_t - X_{t-12} - X_{t-1} + X_{t-13}$\\
      & = & $a+bt+s_t+Y_t -a-b(t-12)-s_{t-12}-Y_{t-12} -a-b(t-1)-s_{t-1}-Y_{t-1} +a+b(t-13)+s_ {t-13}+Y_{t-13}$\\
      & = & $Y_t - Y_{t-1} - Y_{t-12} + Y_{t-13}$
\end{tabular}

So we have $\mu_W(t) = E[W_t] = E[Y_t - Y_{t-1} - Y_{t-12} + Y_{t-13}] = 0$ does not depend on $t$.

We also have

\begin{tabular}{ccl}
$\gamma_W(t, t+h)$ & = & $Cov(W_t, W_{t+h})$\\
& = & $Cov(Y_t - Y_{t-1} - Y_{t-12} + Y_{t-13}, Y_{t+h} - Y_{t+h-1} - Y_{t+h-12} + Y_{t+h-13})$\\
& = & $Cov(Y_t, Y_{t+h}) - Cov(Y_t, Y_{t+h-1}) - Cov(Y_t, Y_{t+h-12}) + Cov(Y_t, Y_{t+h-13})$\\
&  & - $Cov(Y_{t-1}, Y_{t+h}) + Cov(Y_{t-1}, Y_{t+h-1}) + Cov(Y_{t-1}, Y_{t+h-12}) - Cov(Y_{t-1}, Y_{t+h-13})$\\
& & - $Cov(Y_{t-12}, Y_{t+h}) + Cov(Y_{t-12}, Y_{t+h-1}) + Cov(Y_{t-12}, Y_{t+h-12}) - Cov(Y_{t-12}, Y_{t+h-13})$\\
& & + $Cov(Y_{t-13}, Y_{t+h}) - Cov(Y_{t-13}, Y_{t+h-1}) - Cov(Y_{t-13}, Y_{t+h-12}) + Cov(Y_{t-13}, Y_{t+h-13})$\\
& = & $\gamma_Y(h) - \gamma_Y(h-1) - \gamma_Y(h-12) + \gamma_Y(h-13)$\\
& & $- \gamma_Y(h+1) + \gamma_Y(h) + \gamma_Y(h-11) - \gamma_Y(h-12)$\\
& & $- \gamma_Y(h+12) + \gamma_Y(h+11) + \gamma_Y(h) - \gamma_Y(h-1)$\\
& & $+ \gamma_Y(h+13) - \gamma_Y(h+12) - \gamma_Y(h+1) + \gamma_Y(h)$\\
& = & $4\gamma_Y(h) -2\gamma_Y(h-1) -2\gamma_Y(h-12) +\gamma_Y(h-13)$\\
& & $-2\gamma_Y(h+1) +\gamma_Y(h-11) -2\gamma_Y(h+12) +\gamma_Y(h+11) +\gamma_Y(h+13)$
\end{tabular}

So we get that $\gamma_W(t, t+h)$ does not depend on $t$.

Therefore, we can conclude that the process $\{W_t\}$ is stationary.

\color{black}

b. If $X_t = (a + bt)s_t + Y_t$, where $s_t$ is a seasonal component with period 12, show that $\nabla^2_{12} X_t = (1 - B^{12})^2 X_t$ is stationary and express its autocovariance function in terms of that of $\{Y_t\}$.

\color{blue}
Let's call $Z_t = \nabla_{12}^2 X_t$.

We have

\begin{tabular}{ccl}
$Z_t$ & = & $\nabla_{12}^2 X_t$\\
      & = & $(1-B^{12})^2X_t$\\
      & = & $(1-B^{12})(1-B^{12})X_t$\\
      & = & $(1-B^{12})(X_t - X_{t-12})$\\
      & = & $X_t - X_{t-12} - X_{t-12} + X_{t-24}$\\
      & = & $X_t - 2X_{t-12} + X_{t-24}$\\
      & = & $(a+bt)s_t+Y_t -2((a+b(t-12))s_{t-12}+Y_{t-12}) +(a+b(t-24))s_{t-24} + Y_{t-24}$\\
      & = & $a(s_t-2s_{t-12}+s_{t-24}) + b(ts_t-2(t-12)s_{t-12}+(t-24)s_{t-24})+Y_t-2Y_{t-12}+Y_{t-24}$\\
      & = & $Y_t-2Y_{t-12}+Y_{t-24}$
\end{tabular}

So we have $\mu_Z(t) = E[Z_t] = E[Y_t-2Y_{t-12}+Y_{t-24}] = 0$ does not depend on $t$.

We also have

\begin{tabular}{ccl}
$\gamma_Z(t, t+h)$ & = & $Cov(Z_t, Z_{t+h})$\\
& = & $Cov(Y_t-2Y_{t-12}+Y_{t-24}, Y_{t+h}-2Y_{t+h-12}+Y_{t+h-24})$\\
& = & $Cov(Y_t, Y_{t+h}) -2Cov(Y_t, Y_{t+h-12}) +Cov(Y_t, Y_{t+h-24})$\\
&   & $-2Cov(Y_{t-12}, Y_{t+h}) +4Cov(Y_{t-12}, Y_{t+h-12}) -2Cov(Y_{t-12}, Y_{t+h-24})$\\
&   & $+ Cov(Y_{t-24}, Y_{t+h}) -2Cov(Y_{t-24}, Y_{t+h-12}) +Cov(Y_{t-24}, Y_{t+h-24})$\\
& = & $\gamma_Y(h) -2\gamma_Y(h-12) +\gamma_Y(h-24)$\\
&   & $-2\gamma_Y(h+12) +4\gamma_Y(h) -2\gamma_Y(h-12)$\\
&   & $+\gamma_Y(h+24) -2\gamma_Y(h+12) +\gamma_Y(h)$\\
& = & $6\gamma_Y(h) -4\gamma_Y(h-12) +\gamma_Y(h-24) -4\gamma_Y(h+12) +\gamma_Y(h+24)$
\end{tabular}

So we get that $\gamma_Z(t, t+h)$ does not depend on $t$.

Therefore, we can conclude that the process $\{Z_t\}$ is stationary.
\color{black}


\section{Additional Problems}
\subsection{ARIMA fitting with the \texttt{LakeHuron} dataset.}
The \texttt{LakeHuron} dataset contains annual measurements of the level, in feet, of Lake Huron between 1875 and 1972.

(a) Take ordinary differences of the data until the resulting time series is suitably stationary, using the Augmented Dickey-Fuller Test to aid in your choice of d.

\color{blue}
We start by plotting the data and the ACF plot:

\begin{Verbatim}[frame=single]
# Plot original data
par(mfrow=c(2,1))
plot(LakeHuron, main="Height of Lake Huron Through the Years",
     xlab = "Year", ylab = "Height (ft)")
acf(LakeHuron)
adf.test(LakeHuron)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{huronplot.png}
\caption{Plot of the data and ACF plot.}
\label{huronplot}
\end{figure}


We get the plots shown in Figure \ref{huronplot}, where we can see that the process is clearly not stationary. Indeed, the Augmented Dickey-Fuller test produces a p-value of 0.254. So we do not reject the null hypothesis that the time series is not stationary. Therefore, we try differencing once and look at the result:

\begin{Verbatim}[frame=single]
# Difference once
LakeHuron1 <- diff(LakeHuron)
plot(LakeHuron1, main="Differenced Height of Lake Huron Through the Years",
     xlab = "Year", ylab = "Height (ft)")
acf(LakeHuron1)
adf.test(LakeHuron1)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{huron1plot.png}
\caption{Plot of the 1-differenced data and ACF plot.}
\label{huron1plot}
\end{figure}


We get the plots shown in Figure \ref{huron1plot}, where the process seems stationary. Indeed, the Augmented Dickey-Fuller test now produces a p-value $< 0.01$, so we reject the null hypothesis and conclude that the time series is stationary. We do not need to difference again. We conclude that $d=1$.

\color{black}

(b) Fit an $AR(1)$ model to the d-differenced \texttt{LakeHuron} data, using maximum likelihood estimation.

\color{blue}
We fit an AR(1) model to the 1-differenced data:

\begin{Verbatim}[frame=single]
m1 <- arima(LakeHuron1, order = c(1,0,0))
summary(m1)
\end{Verbatim}

For this model, we get $\hat{\sigma}^2 = 0.5452$, log likelihood = -108.23, and aic = 222.45.
\color{black}

(c) Fit an $AR(2)$ model to the d-differenced \texttt{LakeHuron} data, using maximum likelihood estimation.

\color{blue}
We fit an AR(2) model to the 1-differenced data:

\begin{Verbatim}[frame=single]
m2 <- arima(LakeHuron1, order = c(2,0,0))
summary(m2)
\end{Verbatim}

For this model, we get $\hat{\sigma}^2 = 0.5188$, log likelihood = -105.87, and aic = 219.74.
\color{black}

(d) Perform a likelihood ratio test comparing the $AR(1)$ and $AR(2)$ models and state
whether or not the null hypothesis is rejected.

\color{blue}
We perform a Likelihood Ratio Test to compare the two models. Here, the null model is \texttt{m1}, and the alternative model is \texttt{m2}:

\begin{Verbatim}[frame=single]
# Compare the two models using LRT
D <- -2*(m1$loglik - m2$loglik)
pval <- 1-pchisq(D,1)
print(c("Test Statistic:",round(D,4),"P-value:",round(pval,4)))
\end{Verbatim}

We get a p-value of 0.0299, so we reject the null hypothesis that the two models fit equally well, and accept the alternative that the AR(2) model fits better.
\color{black}

(e) Using your results from (d) and an assessment of $AIC$ and $\hat{\sigma}^2$ for each model, state which model you deem to be “optimal” and briefly explain this choice.

\color{blue}
From the result in (d), we are inclined to choose the AR(2) model \texttt{m2}. By comparing $AIC$ and $\hat{\sigma}^2$, we see that they are lower for \texttt{m2}, confirming that this model fits better.
\color{black}

(f) Using appropriate formal and informal residual diagnostics, investigate whether the “optimal” model you’ve chosen in (e) satisfies the following assumptions
\begin{itemize}
  \item Zero-Mean
  \item Homoscedasticity
  \item Zero-Correlation
  \item Normality
\end{itemize}

\color{blue}
We start by extracting the residuals from the model:

\begin{Verbatim}[frame=single]
# Residual diagnostics
e <- m2$residuals # residuals
r <- e/sqrt(m2$sigma2) # standardized residuals
\end{Verbatim}

Then, we plot the residuals against time. The horizontal line at 0 makes it easier to see whether the data seems to have zero-mean.
\begin{Verbatim}[frame=single]
# Plot residuals vs t
par(mfrow=c(2,1))
plot(e, main="Residuals vs t", ylab="")
abline(h=0, col="red")
plot(r, main="Standardized Residuals vs t", ylab="")
abline(h=0, col="red")

# test whether residuals have zero mean
t.test(e)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{huronseq.png}
\caption{Sequential plot of residuals and standardized residuals.}
\label{huronseq}
\end{figure}

The plots in Figure \ref{huronseq} show that the process seems to have zero-mean. We can also say that it looks homoskedastic, and we can't discern any correlation nor outliers. The result from the t-test produces a p-value of 0.9888, so we do not reject the null hypothesis that the mean is zero.

In order to test Homoskedasticity, we split our data into four groups of similar length, as seen in Figure \ref{huronhomo}. Then we use Bartlett's and Levene's tests to check whether the variance in each group is statistically equal.
\begin{Verbatim}[frame=single]
# test for heteroscedasticity
par(mfrow=c(1,1))
plot(e, main="Residuals vs t", ylab="")
abline(v=c(1899,1923,1947), lwd=3, col="red")
group <- c(rep(1,24),rep(2,24),rep(3,24),rep(4,25))
levene.test(e,group) #Levene 
bartlett.test(e,group) #Bartlett 
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{huronhomo.png}
\caption{Data splitting for homoskedasticity tests.}
\label{huronhomo}
\end{figure}

Bartlett's test produces a p-value of 0.1629, and Levene's test 0.2029. Therefore neither null hypothesis is rejected, and we conclude that the process is homoskedastic.

To formally check for uncorrelatedness, we use the Ljung-Box test:
\begin{Verbatim}[frame=single]
# test for uncorrelatedness / randomness
tsdiag(m1)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{huronljung.png}
\caption{Plots for the Ljung-Box test.}
\label{huronljung}
\end{figure}

The plots in Figure \ref{huronljung} indicate that the data is random/uncorrelated.

Finally, we need to verify if the residuals are normally distributed, since we used Maximum Likely Estimators:
\begin{Verbatim}[frame=single]
# test for normality
par(mfrow=c(1,1))
qqnorm(e, main="QQ-plot of Residuals")
qqline(e, col = "red")
shapiro.test(e) #SW test
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{huronqq.png}
\caption{QQ-plot of the residuals.}
\label{huronqq}
\end{figure}

The QQ-plot in Figure \ref{huronqq} seems OK, and Shapiro-Wilk's test produces a p-value of 0.4233, so we do not reject the null hypothesis that the residuals are normally distributed.

All the residual diagnostics satisfy the assumptions, so we can accept the AR(2) model to model this process.
\color{black}

\newpage
\subsection{SARIMA fitting with the \texttt{beers.csv} dataset.}
The \texttt{beers.csv} dataset contains information on monthly Australian beer production,
in millions of litres, from January 1956 to August 1995.

(a) Using techniques discussed in class, fit (using maximum likelihood estimation) an “optimal” $SARIMA(p,d,q) \times (P,D,Q)$ model to the \texttt{beers.csv} time series, and justify your choice of $p, d, q, P, D, Q$ and $s$.

\color{blue}
We start by importing the data and plotting the series:
\begin{Verbatim}[frame=single]
# Load data
beer <- read.csv("beer.csv", header=T)
beer <- ts(beer, start = c(1956,1), frequency = 12)
plot(beer, main="Beer Production in Australia",
     xlab="Year", ylab="Production (x1e6 L)")
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{beer.png}
\caption{Plot of beer data set.}
\label{beer}
\end{figure}

The term \texttt{frequency=12} is used to indicate that the data is separated by months. We get the plot shown in Figure \ref{beer}. We can see that there seems to be a polynomial trend as well as seasonality in the process. In addition, the amplitude of the variations seem to increase with time, so we may think about doing a log transformation on the data. This is what we do, and we check the resulting plot:
\begin{Verbatim}[frame=single]
# Take log-transform of the data
lbeer <- log(beer)
plot(lbeer, main="Beer Production in Australia",
     xlab="Year", ylab="Log of Production (x1e6 L)")
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeer.png}
\caption{Plot of log-transformed beer data set.}
\label{lbeer}
\end{figure}

As seen in Figure \ref{lbeer}, the amplitude of the oscillations now seems more even throughout time.

We can then look at the ACF plot of the series to check for ordinary and/or seasonal differencing that may be necessary:
\begin{Verbatim}[frame=single]
# check whether ordinary and/or seasonal differencing is necessary
acf(lbeer, lag.max = 96)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeeracf.png}
\caption{ACF of log-transformed beer data set.}
\label{lbeeracf}
\end{figure}

Figure \ref{lbeeracf} shows slow decay as well as a sinusoidal pattern. This indicates that both trend and seasonality are present in the data and need to be accounted for. We start with ordinary differencing:

\begin{Verbatim}[frame=single]
# Both forms of differencing seem necessary. Let's do ordinary first:
par(mfrow=c(2,1))
dlbeer <- diff(lbeer)
plot(dlbeer, main="Trend corrected Beer Production in Australia",
     xlab="Year", ylab="Log of Production (x1e6 L)")
acf(dlbeer, lag.max=48)
adf.test(dlbeer)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeertrend.png}
\caption{Trend-corrected log-transformed beer data set.}
\label{lbeertrend}
\end{figure}

The resulting plots shown in Figure \ref{lbeertrend} seem ok. Also, the ADF test produces a p-value inferior to 0.01, so we reject the null hypothesis that the process is not stationary: we do not need to difference again. We have d=1.

Now, we perform seasonal differencing with lag 12 (this value was determined by counting how many "peaks" are in one period in the ACF plot):
\begin{Verbatim}[frame=single]
# Still need seasonal differencing:
dlbeer.12 <- diff(dlbeer, lag=12)
plot(dlbeer.12, main="Trend and Seasonality Corrected Beer Production in Australia",
     xlab="Year", ylab="Log of Production (x1e6 L)")
acf(dlbeer.12, lag.max=48)
adf.test(dlbeer.12, k=12)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeerseas.png}
\caption{Seasonal and trend-corrected log-transformed beer data set.}
\label{lbeerseas}
\end{figure}

The resulting plots shown in Figure \ref{lbeerseas} seem ok. Also, the ADF test produces a p-value inferior to 0.01, so we reject the null hypothesis that the process is not stationary: we do not need to seasonally difference again. We have D=1 and s=12.

Now that we have appropriately differenced, we know that we are fitting a SARIMA model, and therefore need to choose p, P, q, and Q. In order to do so, we look at the ACF and PACF plots of the differences data:
\begin{Verbatim}[frame=single]
# This seems fine now. Since we seasonally differenced, we are fitting a SARIMA model
# and need to choose p, P, q, Q.
# Let's look at the ACF/PACF plots for this
par(mfrow=c(2,1))
acf(dlbeer.12, lag.max=48)
pacf(dlbeer.12, lag.max=48)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeeracfpacf.png}
\caption{ACF and PACF plots of log-transformed beer data set.}
\label{lbeeracfpacf}
\end{figure}

From the two plots shown in Figure \ref{lbeeracfpacf}, we can infer the following hypotheses: $p \leq 5, q \leq 4, P = 0, Q = 1$. Therefore, we fit the SARIMA $(5,1,4)\times(0,1,1)_{12}$ model (more details on the selection of this particular model in the Annex):
\begin{Verbatim}[frame=single]
# Fit model
m <- arima(lbeer, c(3,1,4), list(order=c(0,1,1), period=12))
summary(m)
\end{Verbatim}

This model is fit using Maximum Likelihood Estimation (after finding starting values with Conditional Sum of Squares). For this model, we get the following coefficients and standard errors:

\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
Coefficients&$\phi_1$&$\phi_2$&$\phi_3$&$\theta_1$&$\theta_2$&$\theta_3$&$\theta_4$&$\Theta_1$\\
\hline
Values&-0.3895&-0.1163&0.7648&-0.6452&-0.2173&-0.8401&0.8296&-0.8385\\
S.E.&0.0627&0.0724&0.0627&0.0489&0.0156&0.0172&0.0459&0.0297\\
\end{tabular}
\end{center}

In addition, we have $\hat{\sigma}^2 = 0.003702$, log-likelihood = 626.99, AIC = -1235.99.

We can now visualize how this model fits our data:
\begin{Verbatim}[frame=single]
# Let's visualize how well this model fits
f <- lbeer - m$residuals # fitted values
par(mfrow=c(1,1))
plot(beer, main="Beer Production in Australia",
     xlab="Year", ylab="Production (x1e6 L)")
lines(exp(f), col="red")
legend("bottomright", legend = c("Observed", "Predicted"),
       lty = 1, col = c("black", "red"))
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{beerpred.png}
\caption{Plot of original beer data set and model predictions.}
\label{beerpred}
\end{figure}

From Figure \ref{beerpred}, we can see that this model fits the data well. The predicted values closely follow the original data.

\color{black}

(b) Using appropriate formal and informal residual diagnostics, investigate whether the “optimal” model you’ve found in (a) satisfies the following assumptions
\begin{itemize}
  \item Zero-Mean
  \item Homoscedasticity
  \item Zero-Correlation
  \item Normality
\end{itemize}

\color{blue}
We start by extracting the residuals from the model:
\begin{Verbatim}[frame=single]
# Residual diagnostics
e <- m$residuals # residuals
r <- e/sqrt(m$sigma2) # standardized residuals
\end{Verbatim}

Then, we plot the residuals against time. The horizontal line at 0 makes it easier to see whether the data seems to have zero-mean.
\begin{Verbatim}[frame=single]
# Plot residuals vs t
par(mfrow=c(2,1))
plot(e, main="Residuals vs t", ylab="")
abline(h=0, col="red")
plot(r, main="Standardized Residuals vs t", ylab="")
abline(h=0, col="red")

# test whether residuals have zero mean
t.test(e)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeerseq.png}
\caption{Sequential plot of residuals and standardized residuals.}
\label{lbeerseq}
\end{figure}

The plots in Figure \ref{lbeerseq} show that the process seems to have zero-mean, with maybe one issue around 1982. We can't discern any correlation nor outliers, except again for one observation around 1982. The result from the t-test produces a p-value of 0.7775, so we do not reject the null hypothesis that the mean is zero.

In order to test Homoskedasticity, we split our data into four groups of similar length, as shown in Figure \ref{lbeerhomo}. Then we use Bartlett's and Levene's tests to check whether the variance in each group is statistically equal.
\begin{Verbatim}[frame=single]
# test for heteroscedasticity
par(mfrow=c(1,1))
plot(e, main="Residuals vs t", ylab="")
abline(v=c(1966,1976,1986), lwd=3, col="red")
group <- c(rep(1,119),rep(2,119),rep(3,119),rep(4,119))
levene.test(e,group) #Levene 
bartlett.test(e,group) #Bartlett 
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeerhomo.png}
\caption{Sequential plot of residuals and standardized residuals.}
\label{lbeerhomo}
\end{figure}

Bartlett's test produces a p-value of 0.001243, and Levene's test 0.04535. Both p-values lead us to reject the null hypothesis that the variance is the same between each group. This model fails the homoskedasticity assumption. This may be due to the observation in 1982 that increases the variance in its group (and may also cause issues with normality later on). Informally, by only looking at the sequential plot of the residuals, it does not seem like the data is very heteroskedastic.

To formally check for uncorrelatedness, we use the Ljung-Box test:
\begin{Verbatim}[frame=single]
# test for uncorrelatedness / randomness
tsdiag(m)
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeerljung.png}
\caption{Plots for the Ljung-Box test.}
\label{lbeerljung}
\end{figure}

The plots in Figure \ref{lbeerljung} indicate that the data is random/uncorrelated.

Finally, we need to verify if the residuals are normally distributed, since we used Maximum Likely Estimators:
\begin{Verbatim}[frame=single]
# test for normality
par(mfrow=c(1,1))
qqnorm(e, main="QQ-plot of Residuals")
qqline(e, col = "red")
shapiro.test(e) #SW test
# Histogram
hist(e, freq = F, xlim = c(-.4,.4), main = "Histogram of residuals")
curve(dnorm(x, mean(e), sd(e)), col="darkblue", lwd=1, add=TRUE, yaxt="n")
legend("topright", legend=c("N(-0.001, 0.004)"), lty = c(1), lwd=c(1),
       col=c("darkblue"))
\end{Verbatim}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeerqq.png}
\caption{QQ-plot of the residuals.}
\label{lbeerqq}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{lbeerhist.png}
\caption{Histogram of residuals.}
\label{lbeerhist}
\end{figure}

The QQ-plot in Figure \ref{lbeerqq} seems OK, except on the lower tail, where some points start to drift away from the line. Shapiro-Wilk's test produces a p-value of 0.0001, so we are inclined to reject the null hypothesis that the residuals are normally distributed. The histogram on Figure \ref{lbeerhist} however suggests that the process does not deviate too much from normality. The low p-value observed from Shapiro-Wilk's test may be due to some outlier observations. On this basis, we can choose to accept normality for this model.

Therefore, we have seen that aside from possible heteroskedasticity, the SARIMA $(5,1,4)\times(0,1,1)_{12}$ model satisfies the residual diagnostics (at least informally). This seems like an adequate choice to model this process. 

\color{black}

\newpage
\section{Annex}
\color{blue}
In order to select the right $p, q$, we fit all possible models such that $p\leq5$ and $q\leq5$, then compare $\hat{\sigma}^2$, log-likelihood, and AIC in order to choose the "best" one:

\begin{Verbatim}[frame=single]
# Compare different models
m0<-arima(lbeer, c(1,1,1), list(order=c(0,1,1), period=12))
m1<-arima(lbeer, c(1,1,2), list(order=c(0,1,1), period=12))
m2<-arima(lbeer, c(1,1,3), list(order=c(0,1,1), period=12))
m3<-arima(lbeer, c(1,1,4), list(order=c(0,1,1), period=12))
m4<-arima(lbeer, c(2,1,1), list(order=c(0,1,1), period=12))
m5<-arima(lbeer, c(2,1,2), list(order=c(0,1,1), period=12))
m6<-arima(lbeer, c(2,1,3), list(order=c(0,1,1), period=12))
m7<-arima(lbeer, c(2,1,4), list(order=c(0,1,1), period=12))
m8<-arima(lbeer, c(3,1,1), list(order=c(0,1,1), period=12))
m9<-arima(lbeer, c(3,1,2), list(order=c(0,1,1), period=12))
m10<-arima(lbeer, c(3,1,3), list(order=c(0,1,1), period=12))
m11<-arima(lbeer, c(3,1,4), list(order=c(0,1,1), period=12))
m12<-arima(lbeer, c(4,1,1), list(order=c(0,1,1), period=12))
m13<-arima(lbeer, c(4,1,2), list(order=c(0,1,1), period=12))
m14<-arima(lbeer, c(4,1,3), list(order=c(0,1,1), period=12))
m15<-arima(lbeer, c(4,1,4), list(order=c(0,1,1), period=12))
m16<-arima(lbeer, c(5,1,1), list(order=c(0,1,1), period=12))
m17<-arima(lbeer, c(5,1,2), list(order=c(0,1,1), period=12))
m18<-arima(lbeer, c(5,1,3), list(order=c(0,1,1), period=12))
m19<-arima(lbeer, c(5,1,4), list(order=c(0,1,1), period=12))

sigma2<-c(m0$sigma2,m1$sigma2,m2$sigma2,m3$sigma2,m4$sigma2,m5$sigma2,m6$sigma2,m7$sigma2,
          m8$sigma2,m9$sigma2,m10$sigma2,m11$sigma2,m12$sigma2,m13$sigma2,m14$sigma2,
          m15$sigma2,m16$sigma2,m17$sigma2,m18$sigma2,m19$sigma2)
loglik<-c(m0$loglik,m1$loglik,m2$loglik,m3$loglik,m4$loglik,m5$loglik,m6$loglik,m7$loglik,
          m8$loglik,m9$loglik,m10$loglik,m11$loglik,m12$loglik,m13$loglik,m14$loglik,
          m15$loglik,m16$loglik,m17$loglik,m18$loglik,m19$loglik)
AIC<-c(m0$aic,m1$aic,m2$aic,m3$aic,m4$aic,m5$aic,m6$aic,m7$aic,m8$aic,m9$aic,
       m10$aic,m11$aic,m12$aic,m13$aic,m14$aic,m15$aic,m16$aic,m17$aic,m18$aic,m19$aic)
d <- data.frame(pdq = c("(1,1,1)","(1,1,2)","(1,1,3)","(1,1,4)",
                       "(2,1,1)","(2,1,2)","(2,1,3)","(2,1,4)",
                       "(3,1,1)","(3,1,2)","(3,1,3)","(3,1,4)",
                       "(4,1,1)","(4,1,2)","(4,1,3)","(4,1,4)",
                       "(5,1,1)","(5,1,2)","(5,1,3)","(5,1,4)"),sigma2,loglik,AIC)

# Order this by sigma2
d[order(d$sigma2),]

# Order this by loglik
d[order(-d$loglik),]

# Order this by AIC
d[order(d$AIC),]
\end{Verbatim}

By looking at the different orderings of the table \texttt{d}, by ascending $\hat{\sigma}^2$, descending log-likelihood, and ascending AIC, we obtain four models that consitently give the best results: $(5,1,4)\times(0,1,1)_{12}$, $(4,1,4)\times(0,1,1)_{12}$, $(3,1,4)\times(0,1,1)_{12}$, and $(4,1,3)\times(0,1,1)_{12}$. We use Likelihood Ratio Tests in order to determine which model is the "best".

First, we compare $(4,1,4)\times(0,1,1)_{12}$ with $(5,1,4)\times(0,1,1)_{12}$. The p-value obtained is 0.7521, so we do not reject the null hypothesis that the models fit equally well. We keep the simpler one. Then, we compare $(3,1,4)\times(0,1,1)_{12}$ with $(4,1,4)\times(0,1,1)_{12}$. The p-value is 0.2111, so we do not reject the null hypothesis that the models fit equally well. We keep the simpler one. We cannot compare $(3,1,4)\times(0,1,1)_{12}$ with $(4,1,3)\times(0,1,1)_{12}$ since they have the same number of parameters. Finally, we compare $(3,1,4)\times(0,1,1)_{12}$ with $(2,1,4)\times(0,1,1)_{12}$. The p-value is less than 0.01, so we reject the null hypothesis that the models fit equally well, and keep the one with more parameters, i.e. $(3,1,4)\times(0,1,1)_{12}$.

\begin{Verbatim}[frame=single]
# Likelihood Ratio Tests
D <- -2*(m15$loglik - m19$loglik)
pval <- 1-pchisq(D,2)
print(c("Test Statistic:",D,"P-value:",pval)) #choose m15

D <- -2*(m11$loglik - m15$loglik)
pval <- 1-pchisq(D,2)
print(c("Test Statistic:",D,"P-value:",pval)) #choose m11

# Cannot compare m11 to m14 because they have the same number of parameters

D <- -2*(m7$loglik - m11$loglik)
pval <- 1-pchisq(D,2)
print(c("Test Statistic:",D,"P-value:",pval)) #choose m11
\end{Verbatim}


\color{black}

\end{document}
